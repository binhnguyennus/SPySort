{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##What is SPySort?\n",
      "\n",
      "SPySort is a spike sorting package written entirely in Python. It takes advantage of Numpy, Scipy, Matplotlib, Pandas and Scikit-learn. Below, you can find a brief how-to-use tutorial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Load the data\n",
      "To begin with, we have to load our raw data. This can be done either by using the **import_data** module of SPySort or by using a custom loading function. In the case where the user would like to use the SPySort's methods to load its data he or she has to take into account the fact that the raw data must be in plain binary format. SPySort does not yet support reading other formats such as HDF5. Here we examine the case where the user decides to use the SPySort's methods. In the demonstration case below we have already the data available on our machine. If you would like to get the data you can use the following commands before performing any further analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Download the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from urllib import urlretrieve\n",
      "\n",
      "data_names = ['Locust_' + str(i) + '.dat.gz' for i in range(1, 5)]\n",
      "data_src = ['http://xtof.disque.math.cnrs.fr/data/'+ n for n in data_names]\n",
      "\n",
      "[urlretrieve(data_src[i], data_names[i]) for i in range(4)] \n",
      "\n",
      "[os.system('gunzip ' + n) for n in data_names]\n",
      "\n",
      "data_files_names = ['Locust_' + str(i) + '.dat' for i in range(1, 5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Load the data\n",
      "Once we have downloaded the data, we load them using the module **import_data**. This module provides the method **read_data(filenames, frequency)**, which loads all the raw data. This method takes as input the filenames and the sampling frequency. \n",
      "\n",
      "At a next level, we can print the five numbers for each row (recording channel) using the method **five_numbers()**. The first column contains the minimal value; the second, the first quartile; the third, the median; the fourth, the third quartile; the fifth, the maximal value. Using these five numbers we can ensure that our data are proper for any further statistical analysis. Moreover, we check if some processing like a division by the standard deviation (SD) has been applied on our raw data by calling the method **checkStdDiv()**. Finally, we can obtain the size of the digitization set by calling the method **discreteStepAmpl()**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from spysort.ReadData import import_data\n",
      "\n",
      "# Data analysis parameter\n",
      "freq = 1.5e4    # Sampling frequency\n",
      "win = np.array([1., 1., 1., 1., 1.])/5.  # Boxcar filter\n",
      "\n",
      "# read_data instance\n",
      "r_data = import_data.read_data(data_files_names, freq)   # Read raw data\n",
      "\n",
      "print r_data.five_numbers()      # Prints the five numbers\n",
      "\n",
      "print r_data.checkStdDiv()       # Check if our data have been divided by std\n",
      "\n",
      "print r_data.discretStepAmpl()   # Print the discretization step amplitude "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'read_data' object has no attribute 'five_numbers'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-c2dc08ca0824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_files_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Read raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mr_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfive_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Prints the five numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Data normalization using MAD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'read_data' object has no attribute 'five_numbers'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Preprossecing the data\n",
      "Once we have loaded the raw data and we have check that the data range (maximum - minimum) is similar (close to 20) on the four recording sites and the inter-quartiles ranges are also similar in previously described five-numbers statistics, we can proceed to the renormalization of our data.\n",
      "\n",
      "The renormalization can be done by calling the method **renormalization()** of the **import_data** module.\n",
      "As one can notice in the following code snippet, the attribute timeseries is not of dimension one. Indeed, the **timeseries** atribute contains the renormalized data in its first dimension and theis corresponding timestamps in the second dimension. Finally, we can plot the renormalized raw data in order to visually inspect our data. \n",
      "\n",
      "The goal of the renormalization is to scale the raw data such that the noise SD is approximately 1. Since it is not straightforward to obtain a noise SD on data where both signal (i.e., spikes) and noise are present, we use this robust type of statistic for the SD. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data normalization using MAD\n",
      "r_data.timeseries[0] = r_data.renormalization()\n",
      "\n",
      "r_data.plot_data(r_data.data[0:300])  # Plot normalized data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we have renormalized our data, but how we know that MAD does its job? In order to know if MAD indeed works, we can compute the Q-Q plots of the whole traces normalized with the MAD and normalized with the \"classical\" SD. This is implemented in SPySort's **chechMad()** method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r_data.checkMad()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Detect peaks\n",
      "After the normalization of the raw data, we apply a threshold method in spite of detecting peaks (possible spike events) in our raw data. Before, detect any peaks we filter the data slightly using a \"box\" filter of length 3. This means that the data points of the original trace are going to be replaced by the average of themselves with their four nearest neighbors. We will then scale the filtered traces such that the MAD is one on each recording site and keep only the parts of the signal which are above a threshold. The threshold is an argument and can be set by the user easily. \n",
      "\n",
      "The peak detection can be done by creating a **spike_detection** instance and then calling the methods **filtering(threshold, window)** and **peaks(filtered_data, minimalDist, notZero, kind)**. The method **filtering()** takes as arguments a threshold value and the filtering window. The method **peaks()** takes as input arguments the filtered data, the minimal distance between two successive peaks, the smallest value above which the absolute value of the derivative is considered null and the sort of detection method. There are two choices for the detection method. The first one is the aggregate, where \n",
      "the data of all channels are summed up and then the detection is performed on the \n",
      "aggregated data. The second one is the classical method, where a peak detection is performed\n",
      "on each signal separatelly. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from spysort import Events.spikes\n",
      "\n",
      "# Create a spike_detection instance\n",
      "s = spikes.spike_detection(r_data.timeseries)\n",
      "\n",
      "# Filter the data using a boxcar window filter of length 3 and a threshold value 4\n",
      "filtered = s.filtering(4.0, win)\n",
      "\n",
      "# Detect peaks over the filtered data\n",
      "sp0 = s.peaks(filtered, kind='aggregate')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Split the data\n",
      "Depending on the nature of our raw data we can accelerate the spike sorting method by splitting our data into two or more sets. In the present example we split the data into two subsets and early and a later one. In sp0E (early), the number of detected events is: 908. On the other hand in sp0L (late), the number of detected events is: 887. Then we can plot our new filtered data sets and our detected peaks calling the methods **plotFilteredData(data, filteredData, threshold)** and **plotPeaks(data, positions)**, respectively.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We split our peak positions into two subsets (left and right)\n",
      "sp0E = sp0[sp0 <= r_data.data_len/2.]\n",
      "sp0L = sp0[sp0 > r_data.data_len/2.]\n",
      "\n",
      "s.plot_filtered_data(s.data, filtered, 4.)  # Plot the data\n",
      "s.plot_peaks(s.data, sp0E)                   # Plot the peaks of the left subset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Making cuts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Dimension reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Spike \"peeling\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "\n",
      "\n",
      "# from Events import events\n",
      "# from Events import clusters\n",
      "# from Events import alignment\n",
      "\n",
      "    # -------------------------------------------------------\n",
      "\n",
      "    # Test of spike_detection class\n",
      "\n",
      "    # -------------------------------------------------------\n",
      "\n",
      "    # Test of events class\n",
      "    # evts = events.build_events(r_data.data, positions, win, before=14,\n",
      "    #                           after=30)\n",
      "    # evtsE = evts.mk_events()    # Make spike events\n",
      "    # noise = evts.mk_noise()     # Make noise events\n",
      "\n",
      "    # evts.plot_mad_median(evtsE)  # Plot mad and median of the events\n",
      "    # evts.plot_events(evtsE)      # Plot events\n",
      "    # -------------------------------------------------------\n",
      "\n",
      "    # Test PCA and KMeans clustering\n",
      "    # CSize = 10\n",
      "    # c = clusters.pca_clustering(r_data.timeseries[0], positions, win, thr=8,\n",
      "    #                            before=14, after=30)\n",
      "\n",
      "    # print c.pca_variance(10)   # Print the variance of the PCs\n",
      "\n",
      "    # c.plot_mean_pca()         # Plot the mean +- PC\n",
      "    # c.plot_pca_projections()  # Plot the projections of the PCs on the data\n",
      "\n",
      "    # kmeans_clusters = c.KMeans(CSize)    # K-means clustering\n",
      "\n",
      "    # gmm_clusters = c.GMM(10, 'diag')    # GMM clustering\n",
      "\n",
      "    # tree = clusters.bagged_clustering(10, 100, 30)   # Bagged clustering\n",
      "\n",
      "    # c.plot_clusters(gmm_clusters)   # Plot the clusters\n",
      "    # -------------------------------------------------------\n",
      "\n",
      "    # Test alignement of the spike events\n",
      "    # goodEvts = c.goodEvts\n",
      "    # align = alignment.align_events(r_data.timeseries[0], positions, goodEvts,\n",
      "    #                                kmeans_clusters, CSize, win)\n",
      "\n",
      "    # evtsE_noj = [align.mk_aligned_events(align.gcpos[i])\n",
      "    #              for i in range(CSize)]\n",
      "\n",
      "    # centers = {\"Cluster \" + str(i): align.mk_center_dictionary(evtsE_noj[i][1])\n",
      "    #            for i in range(CSize)}\n",
      "\n",
      "    # round0 = [align.classify_and_align_evt(align.positions[i], centers)\n",
      "    #           for i in range(len(align.positions))]\n",
      "\n",
      "    # print len([x[1] for x in round0 if x[0] == '?'])\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}